import matplotlib.pyplot as plt 
import pandas as pd

index = [] #x-coordinate
rpe_values = []

class Enviroment: # Creats the class Enviroment
    
    STATE_TRANSITIONS = {"Cue" : "Wait", 
                         "Wait" : "Got Lit", 
                         "Got Lit" : None}
    
    #Links each state with the state that follows
    
    REWARD_TRANSITIONS = {"Cue" : 0, 
                         "Wait" : 0, 
                         "Got Lit" : 1,
                         None :0}
    
    #Links each state with its associated reward
    
    def __init__(self):
        self.state = "Cue"
        #Begin in state self (cue)
    
    def transition(self):
        state = self.state #Current state
        new_state = Enviroment.STATE_TRANSITIONS[state]
        new_reward = Enviroment.REWARD_TRANSITIONS[new_state]
        self.state = new_state #Updates current state
        return (new_state, new_reward) #New state and new reward defined

class TDAgent():
    def __init__(self, gamma=0.8, alpha=0.08, eta=0.05): #Random Values for g and a
        
        self.V = {"Cue" : 0,
                  "Wait" : 0,
                  "Got Lit" : 0,
                  None : 0}
        
        #initializes V table as zero since number of states so small
        
        self.gamma = gamma
        self.alpha = alpha
        self.eta = eta
        
        # This method implements TD-learning
        
    
    def TD_learn(self, state1, reward1, state2, prev_rpe):
        
        V1 = self.V[state1]
        V2 = self.V[state2]
        g = self.gamma
        a = self.alpha
        h = self.eta
        
        # Calculate RPE and new estimate of V1
        # update it with each rpe
        
        
        rpe = g**3 * (reward1 + V2) + 1 - (V1 + (h * prev_rpe)) #this is where delta p goes
        V1 = V1 + (a * rpe)
        
        # Update the internal value of V1
        self.V[state1] = V1
        
        TDAgent.variable = rpe
        rpe_values.append(rpe)
        prev_rpe = rpe
        
        
def rl_loop(environment, agent): #loops the enviro and agent
    prev_rpe = 10
    while environment.state is not None:
        state = environment.state
        reward = environment.REWARD_TRANSITIONS[state]
        
        # This is the moment in which we learn
        transition = environment.transition()
        new_state = None
        if transition is not None:
            new_state = transition[0]
        
        agent.TD_learn(state, reward, new_state, prev_rpe)

        
def run_trials(environment, agent, n):
    #runs n amount of trials 
    for j in range(n):
        environment.state = "Cue"
        rl_loop(environment, agent) 
        
e = Enviroment()
a = TDAgent()

run_trials(e, a, 1)
lit1 = a.V["Got Lit"]

run_trials(e, a, 10)
lit2 = a.V["Got Lit"]

run_trials(e, a, 100)
lit3 = a.V["Got Lit"]

x = ["Got Lit 1","Got Lit 2","Got Lit 3"] 
y = [lit1,lit2,lit3]  
plt.plot(x, y) 
plt.xlabel('States') 
plt.ylabel('Value V') 
plt.title('V Table at 1, 10, and 100') 

plt.show() 

for i in range (1,334):
    index.append(i)

#print(len(rpe_values))
#print(index)

rpe_df = pd.DataFrame()
rpe_df["epoch"] = index
rpe_df["rpe"] = rpe_values

rpe_df
#plt.plot(x=rpe_df.epoch, y=rpe_df.rpe)
ax = plt.gca()
rpe_df.plot(kind="line", x="epoch", y="rpe", ax=ax)
plt.xlabel("Epoch")
plt.ylabel("RPE")
plt.title("RPE vs. Epoch")
plt.show()

plt.plot(index, rpe_values)
plt.show()
